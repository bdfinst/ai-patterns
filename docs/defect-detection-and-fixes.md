# Defect Cause Catalog

| Category | Defect Cause | Earliest Detection Stage | Automated Detection | Systemic Correction |
| --- | --- | --- | --- | --- |
| **Product & Discovery** | **Building the wrong thing entirely** | Discovery — Before any code. Product analytics and user research during discovery phase. | Track feature adoption rates post-release with product analytics (Amplitude, Mixpanel); alert when new features have <X% usage after N days; automated funnel analysis showing drop-off | Require validated user research before work enters the backlog; dual-track agile with discovery running ahead of delivery; kill features that don't hit adoption thresholds within a defined window |
|  | **Solving a problem nobody has** | Discovery — Before any code. Customer interviews and demand validation. | Monitor support ticket topics vs. feature investment; automated gap analysis between user-reported pain and roadmap items; survey automation (NPS, CSAT) correlated with feature releases | Mandate problem validation (interviews, data analysis) as a stage gate before solution design; publish a 'problem brief' before any 'solution brief'; connect roadmap items to quantified user pain |
|  | **Correct problem, wrong solution** | Discovery — Before any code. Prototype testing during solution design. | A/B test frameworks with automated significance calculation; feature flag analytics comparing cohorts; automated task-completion-rate tracking in usability tools | Default to prototyping multiple approaches before committing; require measurable success criteria before building; run solution experiments with feature flags before full rollout |
|  | **Meets the spec but misses user intent** | Requirements — During story writing. Acceptance criteria review with users. | Session replay tools (FullStory, Hotjar) with automated rage-click and error-loop detection; track task completion rate, not just feature presence; automated UX heuristic scanning | Write acceptance criteria as user outcomes, not functional checklists; include 'how will we know this works for users?' on every story; regular usability testing cadence, not just at launch |
|  | **Over-engineering beyond actual need** | Design — During architecture/design. Review scope against actual requirements. | Track lead time per story point or unit of value; measure lines of code per feature; static analysis for unused abstractions, dead code, and overly complex class hierarchies | YAGNI as a team norm; time-box architecture spikes; require justification for every abstraction layer; review architecture against actual (not projected) scale requirements quarterly |
|  | **Prioritizing the wrong work (opportunity cost)** | Discovery — During prioritization. Cost of delay analysis before work enters backlog. | Automated DORA metrics (deployment frequency, lead time) correlated with business outcomes; track cost of delay; dashboard comparing investment allocation vs. outcome metrics | Weighted shortest job first (WSJF) for prioritization; regular portfolio reviews with outcome data; make opportunity cost visible by publishing what you chose NOT to do and why |
| **Integration & Boundaries** | **Service/component interface mismatches** | CI Pipeline — On every PR/commit. Contract tests and schema validation in CI. | Consumer-driven contract tests in CI (Pact, Spring Cloud Contract); schema validation in pipelines (OpenAPI, protobuf linting); automated API compatibility checks on every PR. Strategy varies by organizational control — see 'Contract Testing Strategies' tab for full breakdown by control level (full, some influence, none) with specific tools and automation complexity. | Contract tests mandatory for every service boundary; API-first design with generated clients; breaking changes require versioning and migration plan. For services you own: consumer-driven contracts or bi-directional contracts via Pact/Pactflow. For internal services you don't own: provider-published specs with consumer validation, or API snapshot testing. For third-party APIs: anti-corruption layers with automated boundary tests and synthetic monitoring. See 'Contract Testing Strategies' tab for detailed guidance by control level. |
|  | **Incorrect assumptions about upstream/downstream behavior** | Design — During system design. Behavioral contract definition before coding. | Chaos engineering (Gremlin, Litmus) injecting failures automatically; synthetic transaction monitoring across service boundaries; alerting on unexpected response codes/shapes | Document behavioral contracts (timeouts, retries, error semantics), not just data schemas; defensive coding at every boundary; circuit breakers and fallback behaviors as defaults |
|  | **Race conditions and timing dependencies** | Pre-Commit — During development. Thread sanitizers and race detectors run locally and in CI. | Thread sanitizers (TSan) and race detectors in CI; load/stress testing with concurrent users in pipeline; fuzz testing for concurrency paths; production anomaly detection for intermittent failures | Design for idempotency by default; prefer queues and event sourcing over shared mutable state; establish lock ordering conventions; code review checklist item for concurrency safety |
|  | **Inconsistent state across distributed components** | Design — During system design. Consistency model selection before implementation. | Automated reconciliation jobs comparing state across services; distributed tracing (Jaeger, Zipkin) with anomaly detection; saga completion monitoring with alerting on stuck/failed sagas | Choose consistency model deliberately per use case and document it; saga pattern with compensating transactions as default for distributed writes; event sourcing for audit and replay |
| **Knowledge & Communication** | **Implicit domain knowledge not captured in code** | Coding — During development. Code review by domain experts; static analysis for magic numbers. | Onboarding time tracking (how long until a new dev ships independently); automated detection of 'magic numbers' and undocumented business rules in static analysis; knowledge-concentration metrics from git (who can change what) | Domain-driven design with ubiquitous language enforced in code reviews; embed business rules in code (not wikis); pair programming across experience levels; rotate ownership regularly |
|  | **Misunderstood or ambiguous requirements** | Requirements — Before coding starts. Example mapping and Three Amigos sessions. | Track defects tagged as 'requirements gap' or 'misunderstanding'; automated BDD spec coverage (Cucumber, SpecFlow) showing untested scenarios; PR review bots flagging stories without acceptance criteria | Three Amigos (dev, test, product) before work starts; example mapping sessions; executable specifications as the source of truth; given/when/then acceptance criteria required on every story |
|  | **Tribal knowledge loss during team turnover** | Coding — Ongoing. Git history analysis for knowledge concentration. | Bus factor analysis from git history (who exclusively owns which areas); automated knowledge-concentration alerts when a single author covers >X% of a codebase area; documentation freshness checks | Pair and mob programming as default; rotate on-call across all services; automate everything that currently requires 'ask Sarah'; living documentation generated from code and tests |
|  | **Different mental models between teams** | Design — During cross-team design. Context mapping and shared glossary reviews. | Track cross-team integration defects separately; automated detection of divergent naming/terminology across codebases; API contract test failures as a proxy for misalignment | Shared domain model with explicit bounded contexts; regular cross-team architecture syncs; explicit context mapping (upstream/downstream relationships documented and reviewed); shared glossary enforced via linting |
| **Change & Complexity** | **Unintended side effects from seemingly isolated changes** | CI Pipeline — On every PR/commit. Automated test suites, mutation testing, impact analysis. | Comprehensive automated test suites (unit, integration, e2e) in CI; mutation testing (Stryker, PIT) to validate test effectiveness; automated change impact analysis flagging affected downstream consumers | Small, focused commits deployed independently; trunk-based development with short-lived branches; feature flags for decoupling deploy from release; require tests that prove the change is isolated |
|  | **Accumulated technical debt and workarounds** | CI Pipeline — On every PR/commit. Static analysis trends and quality gates. | Static analysis trends over time (cyclomatic complexity, code duplication, dependency cycles); defect density by module; track TODO/HACK/FIXME counts; SonarQube quality gate trends | Continuous refactoring as part of every story (boy scout rule); dedicated tech debt budget (e.g. 20% of capacity); make debt visible on dashboards; treat rising complexity as a leading indicator and act on it |
|  | **Feature interactions nobody anticipated** | Staging — During integration testing. Combinatorial testing and feature flag interaction checks. | Combinatorial/pairwise testing automation; feature flag interaction matrix testing; production anomaly detection after releases; automated regression suites covering feature combinations | Feature flags with controlled rollout and interaction awareness; modular design with explicit feature boundaries; canary deployments with automated rollback on anomaly detection |
|  | **Configuration drift between environments** | CI Pipeline — On every infra change. Drift detection and environment comparison in pipeline. | Infrastructure-as-code drift detection (Terraform plan, AWS Config, Pulumi preview); automated environment comparison tools; smoke tests running in every environment after provisioning | All infrastructure as code, no manual changes ever; immutable infrastructure (replace, don't patch); GitOps for environment management; identical provisioning for all environments from the same source |
| **Testing & Observability Gaps** | **Untested edge cases and error paths** | CI Pipeline — On every PR/commit. Mutation testing and branch coverage gates. | Mutation testing showing surviving mutants; branch coverage (not just line coverage) in CI with minimum thresholds; property-based testing (Hypothesis, fast-check) generating edge cases automatically | Require tests for every bug fix proving the fix works; property-based testing as a standard practice; boundary value analysis in test design; mutation testing scores as a quality gate |
|  | **Missing integration/contract tests at service boundaries** | CI Pipeline — On every PR/commit. Boundary inventory audit; CI fails if contract tests missing. | Automated service boundary inventory vs. contract test inventory; CI pipeline that fails if a new service endpoint lacks contract tests; dependency graph analysis showing untested edges. Audit each boundary for organizational control level (full/some/none) and verify appropriate testing strategy is in place — see 'Contract Testing Strategies' tab. | Contract tests mandatory for every new service boundary — but the type of contract test depends on control level. Full control: CDC with Pact broker as central registry. Some influence: negotiated SLAs with integration tests. No control: consumer-side expectations + synthetic monitoring + anti-corruption layers. Alert when boundaries are added without tests. See 'Contract Testing Strategies' tab for full decision matrix. |
|  | **Insufficient monitoring — defects escape undetected** | Design — During service design. Production readiness checklist before launch. | Observability coverage scoring (what % of services have dashboards, alerts, SLOs); automated checks for services missing health endpoints, structured logging, or trace propagation; SLO burn rate alerting | Observability as a non-functional requirement on every service; production readiness checklist enforced before launch; SLOs defined for every user-facing path; on-call reviews that surface monitoring gaps |
|  | **Test environments that don't reflect production** | CI Pipeline — On every environment change. Automated parity checks in pipeline. | Automated environment parity checks (compare versions, configs, schemas); synthetic transaction comparison across environments; infrastructure diff tools | Same provisioning code for all environments; production-like data in staging (anonymized); containerization for consistency; test in production with feature flags and canary releases |
| **Process & Deployment** | **Long-lived branches diverging from trunk** | Pre-Commit — Continuously. Branch age monitoring with alerts before merge. | Automated branch age tracking with alerts (e.g. branch >1 day old); merge conflict frequency metrics; CI dashboard showing branch count and divergence | Trunk-based development as the default; if branches exist, merge at least daily; CI rejects PRs from branches older than N hours; eliminate the need for feature branches via feature flags |
|  | **Manual steps in build/deploy pipelines** | CI Pipeline — On pipeline changes. Pipeline audit for manual steps; lead time tracking. | Pipeline audit for manual gates/approvals; track deployment lead time (manual steps show as wait time); automated pipeline topology analysis showing non-automated steps | Automate every step from commit to production; manual approvals only for regulatory requirements; treat pipeline automation as a first-class product; alert on any new manual step added |
|  | **Batching too many changes into a single release** | CI Pipeline — On every deploy. Changes-per-deploy metrics with threshold alerts. | Track changes-per-deploy (commits, stories, PRs per release); deployment frequency metrics (DORA); automated alerts when batch size exceeds threshold | Continuous delivery — every commit is a release candidate; single-piece flow; decouple deploy from release using feature flags; small PRs as a team norm with size limits in CI |
|  | **Inadequate rollback capability** | CI Pipeline — On every deploy. Automated rollback testing as part of deployment pipeline. | Automated rollback testing (deploy, rollback, verify in CI); track mean time to rollback; chaos testing that triggers rollback procedures; database migration reversibility checks | Blue/green or canary deployments as default; backward-compatible database migrations only; automated rollback on health check failure; practice rollbacks regularly, not just in emergencies |
| **Data & State** | **Schema migration and backward compatibility failures** | CI Pipeline — On every PR/commit. Schema compatibility checks in CI before merge. | Automated schema compatibility checks in CI (Avro, protobuf compatibility modes); migration dry-runs against production-like data; backward compatibility test suites | Expand-then-contract migration pattern always; never deploy a breaking schema change; schema registry with compatibility enforcement; migrations tested against a copy of production data |
|  | **Null/missing data assumptions** | Pre-Commit — During development. Static analysis for null safety; strict type checking. | Static analysis for null safety (NullAway, TypeScript strict mode, Kotlin null checks); automated test generation for null inputs; production error monitoring for NullPointerException/undefined errors | Use type systems that enforce null safety; Option/Maybe types as default; validate data at system boundaries; never assume upstream data is complete — assert and handle explicitly |
|  | **Concurrency and ordering issues** | CI Pipeline — On every PR/commit. Thread sanitizers; load tests with randomized timing. | Thread sanitizers and race detectors in CI; load tests with randomized timing; production monitoring for out-of-order processing; idempotency verification tests | Design for out-of-order delivery by default; idempotent consumers; version vectors or sequence numbers on events; prefer immutable data structures |
|  | **Cache invalidation errors** | Staging — During integration testing. Cache consistency monitoring; TTL verification. | Cache consistency monitoring (compare cached vs. source values); automated TTL verification; synthetic transactions that detect stale data; alerting on cache hit rates anomalies | Prefer short TTLs over complex invalidation logic; event-driven cache invalidation; cache-aside pattern with explicit invalidation on writes; monitor and alert on staleness, not just hit rates |
| **Dependency & Infrastructure** | **Third-party library upgrades with breaking changes** | CI Pipeline — On dependency updates. Automated upgrade PRs with full test suite execution. | Automated dependency update PRs (Dependabot, Renovate) with full test suite execution; SCA tools flagging breaking version bumps; lock file drift detection | Pin dependencies explicitly; automated upgrade PRs with test gates; evaluate libraries for API stability before adoption; maintain an abstraction layer over volatile dependencies |
|  | **Infrastructure differences across environments** | CI Pipeline — On every infra change. Drift detection; config comparison in pipeline. | Infrastructure-as-code drift detection; automated config comparison across environments; environment parity scoring dashboards | Single source of truth for all environment definitions; immutable infrastructure; containerization (Docker) for app-level parity; GitOps with environment promotion, not separate definitions |
|  | **Network partitions and partial failures handled incorrectly** | Staging — During chaos/load testing. Fault injection and synthetic transaction monitoring. | Chaos engineering injecting network failures (partitions, latency, packet loss); synthetic transaction monitoring; circuit breaker state monitoring and alerting | Design for failure — circuit breakers, retries with backoff, bulkheads as defaults; test failure modes explicitly; runbooks for every known failure mode; game days practicing failure scenarios |

# Contract Testing Strategies

| Organizational Control | Testing Approach | How It Works | Tools & Implementation | Automation Complexity | Earliest Detection Stage | Strengths | Limitations & Risks |
| --- | --- | --- | --- | --- | --- | --- | --- |
| **Full Control (You own both consumer and provider)** | **Consumer-Driven Contract Tests (CDC)** | Consumer defines the contract (the subset of the provider API it actually uses). Provider verifies it can satisfy all consumer contracts. Contracts live in a shared broker. Both sides run tests in their own CI pipelines independently. | Pact (most mature, polyglot); Spring Cloud Contract (JVM-native); Pactflow for enterprise broker. — CI integration: Consumer publishes contract on every build → Provider verifies on every build → Broker tracks compatibility matrix. | Low–Medium | CI Pipeline — On every PR. Consumer publishes contract → provider verifies in its own CI. | Consumer only tests what it actually needs, not the full API surface; provider gets fast feedback on which consumers would break; enables independent deployability; contracts are living documentation of actual coupling. | Requires team discipline to maintain contracts; initial setup cost for broker infrastructure; contract versioning can get complex with many consumers; doesn't test provider business logic, only shape. |
|  | **Bi-Directional Contract Tests** | Both consumer and provider independently generate their API descriptions (e.g., consumer from mocks, provider from its running API). A broker compares them for compatibility. Neither side writes explicit tests for the other. | Pactflow bi-directional feature; consumer generates Pact file from existing mocks (Wiremock, MSW, etc.) → provider generates OpenAPI spec from running app → Pactflow compares. — Alternative: Specmatic with OpenAPI specs on both sides. | Low | CI Pipeline — On every PR. Both sides generate specs independently → broker compares. | Low adoption friction — teams keep their existing mocking/testing tools; no need to write explicit contract tests; works well when both teams already have good API specs; faster to get started than full CDC. | Less precise than CDC — compares schemas, not behavioral expectations; can miss semantic mismatches (right shape, wrong meaning); requires both sides to keep specs accurate; newer approach with less community tooling. |
|  | **Shared Schema/IDL Validation** | Single source of truth schema (protobuf, Avro, Thrift, GraphQL SDL) that both consumer and provider compile against. CI enforces backward compatibility on every schema change. Code generation ensures type safety. | Protobuf + buf (linting + breaking change detection); Avro with schema registry (Confluent) and compatibility modes (BACKWARD, FORWARD, FULL); GraphQL with schema registry. — buf breaking --against main in CI; Confluent Schema Registry compatibility checks on publish. | Low | Pre-Commit — At compile time. Schema compatibility checked on every change; code generation catches mismatches before commit. | Strongest guarantees for data shape compatibility; breaking changes caught at compile time; code generation eliminates hand-written serialization bugs; single source of truth eliminates drift; mature, well-understood approach. | Only validates data shape, not behavior or semantics; requires all teams to adopt the same IDL; schema evolution rules can be constraining; doesn't catch integration logic bugs; tight coupling to schema tooling. |
| **Some Influence (Internal teams you don't own but can collaborate with)** | **Provider-Published Contracts (Provider-Driven)** | Provider team publishes their API contract (OpenAPI spec, AsyncAPI, etc.). Consumer validates their usage against the published spec. Provider may or may not run consumer contracts, but does commit to the published spec as a stable interface. | OpenAPI spec published to a shared registry or repo; consumer uses Specmatic, Prism, or Schemathesis to test against the spec. — CI: Provider lints/validates spec on every PR → Consumer validates its calls against latest published spec. | Low–Medium | CI Pipeline — On every PR. Consumer validates against published provider spec in CI. | Works without requiring the provider to run your tests; provider spec becomes a stable contract they commit to; consumer gets fast feedback against a real spec; good stepping stone toward full CDC if relationship matures. | Provider may change spec without warning if governance is weak; you're trusting their spec is accurate (it may drift from actual behavior); no guarantee they test against your use cases; one-directional — you catch your breaks, not theirs. |
|  | **API Snapshot / Record-Replay Testing** | Record real API interactions from integration/staging environments. Store as snapshots. Replay against provider in CI to detect changes. Alert when responses diverge from recorded baseline. | Hoverfly, VCR (Ruby), Polly.JS, WireMock recording mode; Karate for API snapshot comparison. — CI: Periodic recording job refreshes snapshots → Consumer tests replay against snapshots → Divergence triggers investigation. | Medium | CI Pipeline — On every build. Replay recorded snapshots and detect divergence. | Captures actual behavior, not just documented contracts; works without any provider cooperation; catches behavioral changes that schema checks miss; good for APIs where you can't get a reliable spec. | Snapshots go stale — need regular refresh cycles; recording from staging may not match production; brittle if API has dynamic content (timestamps, IDs); maintenance burden grows with API surface; doesn't prevent breaks, only detects them. |
|  | **Negotiated SLAs with Integration Tests** | Negotiate a lightweight SLA with the provider team: response time, error rates, uptime, and a stable subset of endpoints. Run integration tests in a shared environment that validate the SLA. Both teams are alerted on failures. | Shared integration environment with synthetic transactions; PagerDuty/OpsGenie for SLA breach alerts; custom integration test suites in CI or scheduled runs. — Governance: SLA document in shared repo; quarterly review meeting; integration test ownership shared or by consumer. | Medium | Staging — During integration testing. Synthetic transactions validate SLA in shared environment. | Formalizes the relationship without requiring contract test infrastructure; SLA breaches create shared accountability; integration tests catch real-world issues including performance; works well with teams that are willing but not yet set up for CDC. | Integration tests are slower and flakier than contract tests; shared environments are fragile and contested; SLAs require ongoing negotiation and review; doesn't catch breaking changes before deployment — only after. |
| **No Control (Third-party APIs, SaaS vendors, public APIs)** | **Consumer-Side Contract Tests (Self-Published Expectations)** | Consumer writes tests defining the exact subset of the third-party API they depend on — expected endpoints, response shapes, status codes. Run against mocks in CI, and periodically verify against the real API. Detect drift before it hits production. | Pact consumer-side only (without provider verification); Specmatic with your own OpenAPI spec of their API; custom test suites with JSON Schema validation. — CI: Unit tests against mocks → Scheduled job (daily/weekly) runs same tests against real API → Alert on divergence. | Medium | CI Pipeline — On every build (against mocks). Scheduled verification against real API (daily/weekly). | You own the definition of what you expect; catches drift before production impact; mocks keep CI fast; scheduled real-API runs give early warning; works with any API regardless of provider cooperation. | Your expectations may be wrong or incomplete; scheduled verification means there's a lag before you detect changes; real-API runs can be rate-limited or costly; no guarantee you learn about deprecations before they happen. |
|  | **API Canary / Synthetic Monitoring** | Continuously run lightweight synthetic transactions against the live third-party API from production (or a dedicated canary environment). Monitor response shape, latency, error rates. Alert on any change from established baseline. | Checkly, Datadog Synthetics, Grafana Synthetic Monitoring, AWS CloudWatch Synthetics; custom scripts in Lambda/Cloud Functions on cron. — Setup: Define critical API paths → Synthetic transactions every 1-5 min → Alert on shape/status/latency deviation. | Low–Medium | Production — Continuous in production. Synthetic transactions every 1–5 minutes detect changes in near-real-time. | Catches issues in near-real-time in the actual production environment; low setup cost for critical paths; monitors availability and performance alongside correctness; works with any API — zero provider cooperation needed. | Only tests the paths you define — can miss edge cases; rate limits and costs for frequent checks; doesn't catch breaking changes before they deploy — only after; limited depth (smoke-test level, not comprehensive). |
|  | **Adapter / Anti-Corruption Layer with Automated Boundary Tests** | Wrap the third-party API in an adapter layer (anti-corruption layer) that translates their model to your domain model. Test the adapter extensively. When the external API changes, only the adapter needs updating — your domain code is insulated. | Adapter pattern implementation in your codebase; comprehensive unit and integration tests on the adapter; Testcontainers or WireMock for simulating the external API in CI. — CI: Adapter unit tests with mocks → Integration tests against simulated API → Scheduled verification against real API. | Medium–High | CI Pipeline — On every PR. Adapter unit/integration tests catch boundary issues before merge. | Insulates your entire codebase from third-party changes; change surface is one adapter, not scattered API calls; adapter tests are fast and stable; enables easy provider substitution; domain model stays clean. | Higher upfront engineering cost; adapter itself can have bugs; still need to detect external changes (via canary/synthetic monitoring); adds a layer of indirection; over-engineering if the integration is simple. |
|  | **Vendor Changelog Monitoring + Automated Regression** | Monitor the third-party vendor's changelog, status page, and developer communications for announced changes. Automatically trigger a regression test suite when changes are detected. Combine with synthetic monitoring for unannounced changes. | RSS/webhook monitoring on vendor changelog and status pages (Feedly, custom Lambda); GitHub Actions or similar triggered by changelog updates; regression suite targeting affected API areas. — Auto: Changelog watcher → triggers targeted regression suite → alerts team with change summary + test results. | Medium–High | Post-Release — When vendor announces changes. Changelog monitor triggers targeted regression suite. | Proactive — catches changes before they affect you; combines vendor communication with automated verification; targeted regression reduces noise; builds institutional knowledge of vendor change patterns. | Vendors don't always announce changes (or announce them late); changelog parsing can be brittle; high maintenance if vendor changes frequently; doesn't help with unannounced breaking changes; requires investment in monitoring infrastructure. |
