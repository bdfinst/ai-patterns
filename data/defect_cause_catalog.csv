Category,Defect Cause,Earliest Detection,Automated Detection,Where AI Adds Value,Use Traditional Tooling,Systemic Correction
Product & Discovery,Building the wrong thing,Discovery,Feature adoption tracking; alert on <X% usage after N days; funnel drop-off analysis,Synthesize user feedback; support tickets; and usage data to surface misalignment signals at volume,Adoption dashboards and funnel analytics (Amplitude; Mixpanel) — straightforward metric tracking,Validated user research before backlog entry; dual-track agile; kill features that miss adoption thresholds
Product & Discovery,Solving a problem nobody has,Discovery,Support ticket topic clustering vs. feature investment; automated survey correlation with releases,Semantic analysis of interview transcripts; forums; and tickets to identify actual vs. assumed pain,Ticket counts; feature request votes; search volume — counting problems for helpdesk/analytics tools,Problem validation as a stage gate; publish problem brief before solution brief; quantify user pain
Product & Discovery,Correct problem; wrong solution,Discovery,A/B test significance; feature flag cohort comparison; task completion rate tracking,Evaluate prototypes against problem definitions; generate alternative solution approaches,A/B frameworks; statistical significance calculators — let the math do the math,Prototype multiple approaches; measurable success criteria before building; experiment with flags first
Product & Discovery,Meets spec but misses user intent,Requirements,Rage-click and error-loop detection (FullStory; Hotjar); task completion rate vs. feature presence,Review acceptance criteria for missing user outcomes; analyze session replays at scale for frustration patterns,UX analytics tools handle rage-click detection and session replay — solved pattern-matching,Acceptance criteria as user outcomes; not functional checklists; regular usability testing cadence
Product & Discovery,Over-engineering beyond need,Design,Lead time per unit of value; LOC per feature; static analysis for unused abstractions and dead code,Flag unnecessary abstraction layers; premature optimization; and complexity vs. actual requirements in code review,Dead code detection; complexity scoring (SonarQube; ESLint) — deterministic static analysis,YAGNI as team norm; time-box spikes; justify every abstraction layer; review architecture vs. actual scale quarterly
Product & Discovery,Prioritizing wrong work,Discovery,DORA metrics correlated with business outcomes; cost of delay dashboards; investment vs. outcome tracking,Synthesize roadmap; customer data; and market signals to surface opportunity cost,WSJF scoring; capacity planning — spreadsheet problems needing accurate inputs; not AI,WSJF for prioritization; regular portfolio reviews with outcome data; publish what you chose NOT to do
Integration & Boundaries,Interface mismatches,CI,Consumer-driven contract tests (Pact); schema validation (OpenAPI; protobuf/buf linting); API compat checks per PR,Predict which consumers break from API changes based on usage patterns when formal contracts don't exist,Contract tests; schema validation; compatibility checks — must be deterministic; not probabilistic,Contract tests mandatory per boundary; API-first with generated clients; version + migration plan before merge
Integration & Boundaries,Wrong assumptions about upstream/downstream,Design,Chaos engineering (Gremlin; Litmus); synthetic transactions across boundaries; unexpected response alerting,Review code/docs to identify undocumented behavioral assumptions (timeouts; retries; error semantics),Fault injection; circuit breaker monitoring; synthetic checks — must be deterministic and repeatable,Document behavioral contracts; not just schemas; defensive coding at boundaries; circuit breakers as default
Integration & Boundaries,Race conditions,Pre-Commit,Thread sanitizers (TSan); load testing with concurrency; fuzz testing; anomaly detection for intermittent failures,Limited — can flag concurrency anti-patterns in PR review; but cannot replace formal detection tools,TSan; race detectors; TLA+; fuzz testing — purpose-built formal tools. This is a tooling problem; not a judgment problem,Design for idempotency; queues over shared mutable state; lock ordering conventions; concurrency review checklist
Integration & Boundaries,Inconsistent distributed state,Design,Reconciliation jobs; distributed tracing with anomaly detection; saga completion monitoring,Review designs for missing compensation logic or consistency model mismatches — useful at design time,Tracing (Jaeger/Zipkin); reconciliation; saga monitoring — continuous operational tools; deterministic,Choose consistency model deliberately per use case; saga with compensating transactions; event sourcing
Knowledge & Communication,Implicit domain knowledge not in code,Coding,Magic number detection in static analysis; git knowledge-concentration metrics; onboarding time tracking,HIGH VALUE: Identify undocumented business rules; missing 'why' in code; and knowledge gaps a new dev would hit,Git history analysis; code ownership metrics (CodeScene; git-fame) — counting/graphing problems,DDD with ubiquitous language; embed rules in code not wikis; pair across experience levels; rotate ownership
Knowledge & Communication,Ambiguous requirements,Requirements,Flag stories without acceptance criteria; BDD spec coverage tracking; defect classification for 'requirements gap',HIGH VALUE: Review requirements for ambiguity; missing edge cases; contradictions; and unstated assumptions; generate test scenarios,BDD coverage reports; story tracking; defect tagging — workflow tools (Jira; ADO) handle categorization,Three Amigos before work starts; example mapping; executable specs as source of truth; given/when/then required
Knowledge & Communication,Tribal knowledge loss,Coding,Bus factor from git history; single-author concentration alerts; documentation freshness checks,Generate documentation from code/tests; flag where docs have drifted from implementation,Git analysis; bus factor scoring; last-modified checks — existing analysis tools handle this,Pair/mob programming as default; rotate on-call; automate tribal knowledge; living docs from code
Knowledge & Communication,Divergent mental models across teams,Design,Cross-team integration defect tracking; divergent naming detection across codebases; contract test failures,HIGH VALUE: Compare terminology and domain models across team codebases to detect semantic mismatches before integration failures,Integration defect counts; contract failure rates — lagging indicators for dashboards,Shared domain model; explicit bounded contexts; regular cross-team syncs; shared glossary enforced via linting
Change & Complexity,Unintended side effects,CI,Automated test suites; mutation testing (Stryker; PIT); change impact analysis flagging downstream consumers,Reason about semantic change impact beyond syntactic dependencies — 'this looks isolated but affects X because Y',Test execution; coverage; mutation scoring — deterministic; must remain so,Small focused commits; trunk-based development; feature flags to decouple deploy from release
Change & Complexity,Accumulated technical debt,CI,Complexity trends over time; duplication scoring; dependency cycles; TODO/HACK counts; SonarQube quality gates,Identify architectural drift; abstraction decay; and calcified workarounds that static analysis misses,Cyclomatic complexity; duplication; dependency cycles — measurement problems solved by static analysis,Refactoring as part of every story; dedicated debt budget; boy scout rule; treat rising complexity as leading indicator
Change & Complexity,Unanticipated feature interactions,Staging,Combinatorial/pairwise testing; feature flag interaction matrix; anomaly detection post-release; regression suites,Reason about feature interactions semantically; flagging conflicts that testing matrices might miss,Pairwise testing tools; flag management (LaunchDarkly); canary with auto-rollback — execution/infrastructure problems,Feature flags with controlled rollout; modular design; canary deployments with auto-rollback on anomaly
Change & Complexity,Configuration drift,CI,IaC drift detection (Terraform plan; Pulumi preview; AWS Config); environment comparison; smoke tests per environment,—,Drift detection and environment comparison are deterministic comparison problems — fully solved by existing tooling,All infrastructure as code; immutable infrastructure; GitOps; identical provisioning from same source
Testing & Observability Gaps,Untested edge cases and error paths,CI,Mutation testing; branch coverage with thresholds; property-based testing (Hypothesis; fast-check),Analyze code paths and generate test cases for untested boundaries and error paths humans overlook,Mutation testing; coverage measurement; property-based frameworks — deterministic execution engines,Tests required for every bug fix; property-based testing as standard; boundary value analysis; mutation scores as gate
Testing & Observability Gaps,Missing contract tests at boundaries,CI,Boundary inventory vs. contract test inventory; CI fails if new endpoint lacks tests; untested dependency graph edges,Identify boundaries lacking tests by understanding semantic service relationships beyond network connections,Boundary inventory; CI config checks; Pact broker — binary question: 'does this boundary have a test?',Contract tests mandatory per new boundary; type varies by control level (see Contract Testing Strategies tab)
Testing & Observability Gaps,Insufficient monitoring,Design,Observability coverage scoring; checks for missing health endpoints; structured logging; trace propagation; SLO burn rates,Review architectures and flag observability gaps against production readiness checklists automatically,Observability platforms (Datadog; Grafana; New Relic); SLO alerting — continuous operational tools,Observability as NFR on every service; production readiness checklist enforced; SLOs for every user-facing path
Testing & Observability Gaps,Test environments don't reflect production,CI,Automated environment parity checks; synthetic transaction comparison across environments; infra diff tools,—,Environment parity is a deterministic comparison problem — IaC diff tools and config comparison handle this,Same provisioning for all environments; production-like data in staging; containerization; test in prod with flags
Process & Deployment,Long-lived branches,Pre-Commit,Branch age alerts; merge conflict frequency; CI dashboard showing branch count and divergence,—,Branch age and conflict frequency are counting problems — git hooks and CI config handle enforcement,Trunk-based development; merge at least daily; CI rejects stale branches; feature flags eliminate branch need
Process & Deployment,Manual pipeline steps,CI,Pipeline audit for manual gates; deployment lead time (manual steps = wait time); pipeline topology analysis,—,Pipeline topology is inspectable without judgment — CI/CD platform analytics show manual steps deterministically,Automate every step commit-to-production; manual approvals only for regulatory; treat pipeline as first-class product
Process & Deployment,Batching too many changes per release,CI,Changes-per-deploy metrics; deployment frequency (DORA); alerts when batch size exceeds threshold,—,Batch size is a counting problem — Sleuth; LinearB; DORA dashboards handle this,Continuous delivery — every commit is a candidate; single-piece flow; decouple deploy from release with flags
Process & Deployment,Inadequate rollback capability,CI,Automated rollback testing in CI; mean time to rollback; migration reversibility checks,—,Rollback capability is testable deterministically — infrastructure verification; health check automation,Blue/green or canary as default; backward-compatible migrations only; auto-rollback on health failure; practice regularly
Data & State,Schema migration / backward compat failures,CI,Schema compatibility checks (Avro; protobuf/buf); migration dry-runs against production-like data,Predict downstream impact by understanding how consumers actually use data beyond formal compatibility,Schema compatibility modes; breaking change detection (buf) — deterministic; must remain so,Expand-then-contract always; never deploy breaking schema changes; schema registry with compat enforcement
Data & State,Null/missing data assumptions,Pre-Commit,Null safety static analysis (NullAway; TypeScript strict); automated null-input test generation; production NPE monitoring,Flag code where optional fields are used without null checks; even in languages without strict null safety,Null safety type systems (Kotlin; TypeScript strict; Rust); static analysis (NullAway) — use the type system first,Enforce null-safe type systems; Option/Maybe as default; validate at boundaries; assert and handle explicitly
Data & State,Concurrency and ordering issues,CI,Thread sanitizers; load tests with randomized timing; idempotency verification tests,—,TSan; race detectors; TLA+; model checkers — concurrency requires exhaustive exploration; not judgment,Design for out-of-order delivery; idempotent consumers; version vectors on events; prefer immutable data
Data & State,Cache invalidation errors,Staging,Cache consistency monitoring (cached vs. source); TTL verification; stale data detection; hit rate anomaly alerts,Review cache invalidation logic for incomplete paths or TTL/change-frequency mismatches at design and code review time,Consistency monitoring; TTL checks; hit rate alerting — operational monitoring handled by observability tools,Short TTLs over complex invalidation; event-driven invalidation; cache-aside with explicit invalidation; alert on staleness
Dependency & Infrastructure,Third-party library breaking changes,CI,Automated upgrade PRs (Dependabot; Renovate) with full test suite; SCA for breaking version bumps; lock file drift,Review changelogs to assess breaking change risk and predict compatibility issues based on your actual API usage,Dependabot; Renovate; SCA; lock file checks — automated dependency management with deterministic test gates,Pin dependencies; automated upgrade PRs with test gates; abstract over volatile dependencies; evaluate stability before adoption
Dependency & Infrastructure,Infrastructure differences across environments,CI,IaC drift detection; config comparison across environments; environment parity scoring,—,Drift detection; config comparison; container verification; GitOps reconciliation — deterministic; fully solved,Single source of truth for all environments; immutable infrastructure; containerization; GitOps with promotion
Dependency & Infrastructure,Network partitions / partial failures handled wrong,Staging,Chaos engineering (Gremlin; Litmus); synthetic transaction monitoring; circuit breaker state monitoring,Review architectures for missing failure patterns (no circuit breaker; no retry/backoff; no bulkhead) at design time,Chaos tools; synthetic monitoring; circuit breaker dashboards — runtime tools; deterministic and repeatable,Design for failure as default; circuit breakers; retries; bulkheads; test failure modes explicitly; game days
